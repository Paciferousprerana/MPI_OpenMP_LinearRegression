{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7rc1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heated-queens"
      },
      "source": [
        "# Implementation of Multiple Linear Regression using MPI and OpenMP"
      ],
      "id": "heated-queens"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "military-proportion"
      },
      "source": [
        "## Objectives"
      ],
      "id": "military-proportion"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "forty-still"
      },
      "source": [
        "* Load the data and perform data pre-processing\n",
        "* Identify the features, target and split the data into train and test\n",
        "* Implement multiple Linear Regression by estimating the coefficients on the given data\n",
        "* Use MPI package to distribute the data and implement `communicator`\n",
        "* Define functions for each objective and make a script (.py) file to execute using MPI command\n",
        "* Use OpenMP component to predict the data and calculate the error on the predicted data\n",
        "* Implement the Linear Regression from `sklearn` and compare the results"
      ],
      "id": "forty-still"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "growing-queens"
      },
      "source": [
        "### Dataset"
      ],
      "id": "growing-queens"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLwz-D_xVT8o"
      },
      "source": [
        "The dataset chosen for this mini-project is [Combined Cycle Power Plant](https://archive.ics.uci.edu/ml/datasets/combined+cycle+power+plant). The dataset is made up of 9568 records and 5 columns. Each record contains the values for Ambient Temperature, Exhaust Vaccum, Ambient Pressure, Relative Humidity and Energy Output. The data was collected over a six year period (2006-11)."
      ],
      "id": "yLwz-D_xVT8o"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "global-savings"
      },
      "source": [
        "**Note:** We will be using the mpi4py Python package for MPI based code implementation"
      ],
      "id": "global-savings"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "designing-marketing",
        "scrolled": true
      },
      "source": [
        "!pip -qq install mpi4py"
      ],
      "id": "designing-marketing",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dedicated-thong"
      },
      "source": [
        "#### Importing Necessary Packages"
      ],
      "id": "dedicated-thong"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reported-acrobat",
        "scrolled": true
      },
      "source": [
        "# Importing pandas\n",
        "import pandas as pd \n",
        "# Importing Numpy\n",
        "import numpy as np \n",
        "# Importing MPI from mpi4py package\n",
        "from mpi4py import MPI \n",
        "# Importing sqrt function from the Math\n",
        "from math import sqrt \n",
        "# Importing Decimal, ROUND_HALF_UP functions from the decimal package\n",
        "from decimal import Decimal, ROUND_HALF_UP \n",
        "import time\n",
        "from sklearn.linear_model import LinearRegression"
      ],
      "id": "reported-acrobat",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "standing-zimbabwe"
      },
      "source": [
        "#### Download the data"
      ],
      "id": "standing-zimbabwe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "early-peace"
      },
      "source": [
        "#### Load data \n"
      ],
      "id": "early-peace"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "differential-vacation",
        "scrolled": true
      },
      "source": [
        "FILENAME = \"/content/PowerPlantData.csv\" # File path\n",
        "def loadData(file):\n",
        "  df = pd.read_csv(file)\n",
        "  df.rename(columns = {'AT':'Ambient Temperature', 'V':'Exhaust Vaccum', 'AP':'Ambient Pressure','RH':'Relative Humidity','PE':'Energy Output'}, inplace = True)\n",
        "  return df\n",
        "\n",
        "df = loadData(FILENAME)\n",
        "df.head()"
      ],
      "id": "differential-vacation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "italian-expense"
      },
      "source": [
        "#### Explore data\n",
        "\n",
        "- checking for the number of rows and columns\n",
        "- summary of the dataset\n",
        "- check for the null values \n",
        "- check for the duplicate values"
      ],
      "id": "italian-expense"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "local-quarter",
        "scrolled": true
      },
      "source": [
        "df.head()"
      ],
      "id": "local-quarter",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def explore_data(df):\n",
        "  rows, columns = df.shape\n",
        "  summary = df.describe()\n",
        "  nullValues = df.isnull().sum()\n",
        "  duplicatedValues = df.duplicated().sum()\n",
        "  return rows, columns, summary, nullValues, duplicatedValues\n",
        "\n",
        "rows, columns, summary, nullValues, duplicatedValues = explore_data(df)\n",
        " \n",
        "print('\\n The shape (rows, columns) of the data set is:', rows, columns)\n",
        "print('Summary of the dataset:\\n', summary)\n",
        "print('\\nNull values in the data set:\\n',nullValues)\n",
        "print('\\nNo. of duplicated rows:', duplicatedValues)\n"
      ],
      "metadata": {
        "id": "P2wE1CwdlnuP"
      },
      "id": "P2wE1CwdlnuP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whole-retailer"
      },
      "source": [
        "#### Handle missing data \n",
        "\n",
        "- Replace the null values with the mean/median/mode - fillna()"
      ],
      "id": "whole-retailer"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "incorporated-child",
        "scrolled": true
      },
      "source": [
        "from numpy.core.fromnumeric import mean\n",
        "def handle_data(df):\n",
        "  for col in df.columns:\n",
        "    if df[col].isnull().sum() == 0:\n",
        "      continue\n",
        "    else:\n",
        "      df[col] = df[col].fillna(value=df[col].mean())\n",
        "  return df\n",
        "\n",
        "df = handle_data(df)\n",
        "df.isnull().sum()"
      ],
      "id": "incorporated-child",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loaded-arbitration"
      },
      "source": [
        "#### Scale the data \n",
        "\n",
        "- standardization of the data  can be performed using the below formula\n",
        "\n",
        "$ (x - mean(x)) / std(x) $ "
      ],
      "id": "loaded-arbitration"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "extraordinary-qatar",
        "scrolled": true
      },
      "source": [
        "# Defining a function to standardize the data\n",
        "def standardize_data(df):\n",
        "  return (df - df.mean())/df.std()\n",
        "\n",
        "\n",
        "df_std = standardize_data(df)\n",
        "df_std.head()"
      ],
      "id": "extraordinary-qatar",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thermal-rehabilitation"
      },
      "source": [
        "#### Feature selection\n",
        "\n",
        "- Features: AmbientTemperature, ExhaustVaccum, AmbientPressure, RelativeHumidity \n",
        "- Target Variable: EnergyOutput"
      ],
      "id": "thermal-rehabilitation"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "terminal-starter",
        "scrolled": true
      },
      "source": [
        "def feature_selection(df):\n",
        "  features = df.drop('Energy Output', axis=1)\n",
        "  target = df['Energy Output']\n",
        "  return features, target\n",
        "\n",
        "X, y = feature_selection(df_std)\n",
        "X.head()"
      ],
      "id": "terminal-starter",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.head()"
      ],
      "metadata": {
        "id": "QwQpeNqyuPOS"
      },
      "id": "QwQpeNqyuPOS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "timely-bruce"
      },
      "source": [
        "#### Correlation \n",
        "\n",
        "Calculate correlation between the variables"
      ],
      "id": "timely-bruce"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "durable-making",
        "scrolled": true
      },
      "source": [
        "df.corr()"
      ],
      "id": "durable-making",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "honest-remainder"
      },
      "source": [
        "#### Estimate the coefficients\n",
        "\n",
        "- Calculate the estimated coefficients using the below formula\n",
        "\n",
        "$ β = (X^T X)^{-1} X^T y $ "
      ],
      "id": "honest-remainder"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dimensional-victory",
        "scrolled": true
      },
      "source": [
        "# Calculating the coeffients\n",
        "def estimate_coeff(X,y):\n",
        "  x_t = X.transpose()\n",
        "  inv_dot_xt = np.linalg.inv(x_t.dot(X)).dot(x_t)\n",
        "  coeff = inv_dot_xt.dot(y)\n",
        "  return coeff\n",
        "\n",
        "coeff = estimate_coeff(X,y)\n",
        "coeff"
      ],
      "id": "dimensional-victory",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "interior-bennett"
      },
      "source": [
        "#### Fit the data to estimate the coefficients\n",
        "\n",
        "- create a dummy column in the features dataframe which is made up of all ones\n",
        "- convert the features dataframe into numpy array\n",
        "- call the estimated coefficients function which is defined above"
      ],
      "id": "interior-bennett"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "local-texas",
        "scrolled": true
      },
      "source": [
        "# defining a fit function\n",
        "def fit(X, y):\n",
        "    X1 = np.hstack((np.ones((len(X),1)),X))\n",
        "    coeff = estimate_coeff(X1,y)\n",
        "    return coeff, X1\n",
        "\n",
        "coeff, new_X = fit(X,y)\n",
        "print('intercept:', coeff[0])\n",
        "print('coeeficients:', coeff[1:])"
      ],
      "id": "local-texas",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_X"
      ],
      "metadata": {
        "id": "Ty8_ErQC0fjC"
      },
      "id": "Ty8_ErQC0fjC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_X.shape"
      ],
      "metadata": {
        "id": "le3mPYzx0yu3"
      },
      "id": "le3mPYzx0yu3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hybrid-quick"
      },
      "source": [
        "#### Predict the data on estimated coefficients\n",
        "\n",
        "- Fit the intercept, coefficients values in the below equation\n",
        "\n",
        "  $y = b_0 + b_1*x + ... + b_i*x_i$"
      ],
      "id": "hybrid-quick"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buried-attention",
        "scrolled": true
      },
      "source": [
        " # fucntion to predict the values\n",
        "def predict(x, intercept, coefficients):\n",
        "    '''\n",
        "    y = b_0 + b_1*x + ... + b_i*x_i\n",
        "    '''\n",
        "    predictions = x.dot(coefficients) + intercept\n",
        "    return predictions"
      ],
      "id": "buried-attention",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = predict(X, coeff[0], coeff[1:])"
      ],
      "metadata": {
        "id": "mt8cu2JoQ5Xl"
      },
      "id": "mt8cu2JoQ5Xl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "metadata": {
        "id": "bebuYGVjYxzz"
      },
      "id": "bebuYGVjYxzz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rolled-consultancy"
      },
      "source": [
        "#### Root mean squared error"
      ],
      "id": "rolled-consultancy"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phantom-alabama",
        "scrolled": true
      },
      "source": [
        "# Define a function to calculate the error\n",
        "def rmse(y, y_pred):\n",
        "  return np.sqrt(np.mean((y - y_pred)**2))"
      ],
      "id": "phantom-alabama",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rms_error = rmse(y, y_pred)\n",
        "rms_error"
      ],
      "metadata": {
        "id": "NrDm8xAZ5R7u"
      },
      "id": "NrDm8xAZ5R7u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "experimental-discrimination"
      },
      "source": [
        "#### Split the data into train and test\n",
        "\n",
        "- Shuffle the data\n",
        "- Consider 70 % of data as a train set and the rest of the data as a test set"
      ],
      "id": "experimental-discrimination"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dangerous-salmon",
        "scrolled": true
      },
      "source": [
        "def train_test_split(x, y, test_size=0.3):\n",
        "  shuffle_idx = np.random.permutation(x.shape[0])\n",
        "  X_shuffled, y_shuffled = x[shuffle_idx], y[shuffle_idx]\n",
        "  count = int((1-test_size)*len(x))\n",
        "  return X_shuffled[:count,:], y_shuffled[:count], X_shuffled[count:, :], y_shuffled[count:]"
      ],
      "id": "dangerous-salmon",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_copy = X.to_numpy()\n",
        "y_copy = y.to_numpy()\n",
        "X_train, y_train, X_test, y_test = train_test_split(X_copy,y_copy, test_size=0.3)"
      ],
      "metadata": {
        "id": "DMB7ymDx7uhU"
      },
      "id": "DMB7ymDx7uhU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, y_train.shape"
      ],
      "metadata": {
        "id": "cRdt4uFz-rk-"
      },
      "id": "cRdt4uFz-rk-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1eR-AwqS_FJN"
      },
      "id": "1eR-AwqS_FJN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hired-uniform"
      },
      "source": [
        "#### Make a script and execute everything in one place using MPI\n",
        "\n",
        "- create a communicator\n",
        "- divide the data into slices\n",
        "- prepare the data in root worker to assign the data to all the workers\n",
        "-scatter and gather the data\n",
        "- !mpirun --allow-run-as-root -np 4 python filename.py"
      ],
      "id": "hired-uniform"
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mpi_reg.py\n",
        "# Importing pandas\n",
        "import pandas as pd \n",
        "# Importing Numpy\n",
        "import numpy as np\n",
        "# Importing sqrt function from the Math\n",
        "from math import sqrt \n",
        "# Importing Decimal, ROUND_HALF_UP functions from the decimal package\n",
        "from decimal import Decimal, ROUND_HALF_UP \n",
        "import time\n",
        "from mpi4py import MPI\n",
        "\n",
        "# Define a function to load the data\n",
        "def loadData(file):\n",
        "  df = pd.read_csv(file)\n",
        "  df.rename(columns = {'AT':'Ambient Temperature', 'V':'Exhaust Vaccum', 'AP':'Ambient Pressure','RH':'Relative Humidity','PE':'Energy Output'}, inplace = True)\n",
        "  return df\n",
        "\n",
        "# Explore data to check for no of rows and columns, summary, null and duplicate values\n",
        "def explore_data(df):\n",
        "  rows, columns = df.shape\n",
        "  summary = df.describe()\n",
        "  nullValues = df.isnull().sum()\n",
        "  duplicatedValues = df.duplicated().sum()\n",
        "  return rows, columns, summary, nullValues, duplicatedValues\n",
        "\n",
        "# Defining a function to standardize the data\n",
        "def standardize_data(df):\n",
        "  return (df - df.mean())/df.std()\n",
        "\n",
        "# function for feature selection\n",
        "def feature_selection(df):\n",
        "  features = df.drop('Energy Output', axis=1)\n",
        "  target = df['Energy Output']\n",
        "  return features, target\n",
        "\n",
        "#split data into train and test \n",
        "def train_test_split(x, y, test_size=0.3):\n",
        "  shuffle_idx = np.random.permutation(x.shape[0])\n",
        "  X_shuffled, y_shuffled = x[shuffle_idx], y[shuffle_idx]\n",
        "  count = int((1-test_size)*len(x))\n",
        "  return X_shuffled[:count,:], y_shuffled[:count], X_shuffled[count:, :], y_shuffled[count:]\n",
        "\n",
        "#Dividing the data into slices for workers\n",
        "def dividing_data(x_train, y_train, size_of_workers):\n",
        "    # Size of the slice\n",
        "    slice_for_each_worker = int(Decimal(x_train.shape[0]/size_of_workers).quantize(Decimal('1.'), rounding = ROUND_HALF_UP))      \n",
        "    print('Slice of data for each worker: {}'.format(slice_for_each_worker))\n",
        "    data_list = []\n",
        "    start=0\n",
        "    end = slice_for_each_worker\n",
        "    for i in range(size_of_workers):\n",
        "      data_list.append(np.hstack((x_train[start:end,:], y_train[start:end].reshape(slice_for_each_worker,1))))\n",
        "      start = end\n",
        "      end+= slice_for_each_worker\n",
        "      #data_list.append(np.hstack((x_train[start:end,:], y_train[start:end].reshape(slice_for_each_worker,1))))\n",
        "    return np.array(data_list)\n",
        "\n",
        "# Calculating the coefficients\n",
        "def estimate_coeff(X,y):\n",
        "  x_t = X.transpose()\n",
        "  # x_xt = x_t.dot(X)\n",
        "  # x_xt_inv = np.linalg.inv(x_xt)\n",
        "  inv_dot_xt = np.linalg.inv(x_t.dot(X)).dot(x_t)\n",
        "  coeff = inv_dot_xt.dot(y)\n",
        "  return coeff\n",
        "\n",
        "# defining a fit function\n",
        "def fit(X, y):\n",
        "    # YOUR CODE HERE\n",
        "    X1 = np.hstack((np.ones((len(X),1)),X))\n",
        "    coeff = estimate_coeff(X1,y)\n",
        "    return coeff, X1\n",
        "\n",
        "def predict(x, intercept, coefficients):\n",
        "    '''\n",
        "    y = b_0 + b_1*x + ... + b_i*x_i\n",
        "    '''\n",
        "    beta = np.concatenate(([intercept], coefficients))\n",
        "    predictions = x.dot(beta)\n",
        "    return predictions\n",
        "\n",
        "# function for rmse\n",
        "def rmse(y, y_pred):\n",
        "  #return ((sum((y-y_pred)**2))/len(y))**0.5\n",
        "  return np.sqrt(np.mean((y - y_pred)**2))\n",
        "\n",
        "\n",
        "# Defining a main function \n",
        "def main():\n",
        "    # creating communicator\n",
        "    comm = MPI.COMM_WORLD\n",
        "    rank = comm.Get_rank() \n",
        "    size = comm.Get_size()\n",
        "\n",
        "    # Load file\n",
        "    FILENAME = \"/content/PowerPlantData.csv\" # File path\n",
        "    df = loadData(FILENAME)\n",
        "    print(df.head())\n",
        "\n",
        "    #Explore data\n",
        "    rows, columns, summary, nullValues, duplicatedValues = explore_data(df)\n",
        "    print('\\n The shape (rows, columns) of the data set is:', rows, columns)\n",
        "    print('Summary of the dataset:\\n', summary)\n",
        "    print('\\nNull values in the data set:\\n',nullValues)\n",
        "    print('\\nNo. of duplicated rows:', duplicatedValues)\n",
        "\n",
        "    #standardize the data\n",
        "    df_std = standardize_data(df)\n",
        "    print('standardized data:\\n',df_std.head())\n",
        "\n",
        "    # feature selection\n",
        "    X, y = feature_selection(df_std)\n",
        "\n",
        "    #train test split \n",
        "    X_copy = X.to_numpy()\n",
        "    y_copy = y.to_numpy()\n",
        "    X_train, y_train, X_test, y_test = train_test_split(X_copy,y_copy, test_size=0.3)\n",
        "    print('shape of X_train:', X_train.shape)\n",
        "    print('shape of y_train:', y_train.shape)\n",
        "    print('shape of X_test:', X_test.shape)\n",
        "    print('shape of y_test:', y_test.shape)\n",
        "\n",
        "    \n",
        "    dataList = None\n",
        "    # master process\n",
        "    if rank == 0:\n",
        "        #dividing the data\n",
        "        dataList = dividing_data(X_train, y_train, size)\n",
        "        #print('rank=', rank, 'datalist=', dataList)\n",
        "    else:\n",
        "        dataList = None\n",
        "    sendbuf = np.empty((1674,5))\n",
        "    comm.Scatter(dataList, sendbuf, root=0)\n",
        "    print('Rank: ',rank, ', sendbuf received: ',sendbuf[:,-1])\n",
        "    #print('data list outside rank 0 =', type(dataList))\n",
        "\n",
        "    rmse_collect = []\n",
        "    comm.Barrier()\n",
        "    X_train_subset = sendbuf[:,:-1]\n",
        "    #print('Rank: ',rank, ', X_train_subset: ',X_train_subset)\n",
        "    y_train_subset = sendbuf[:,-1]\n",
        "    print('Rank: ',rank, ', y_train_subset: ',y_train_subset)\n",
        "    coeff = estimate_coeff(X,y)\n",
        "    coeff, new_X = fit(X,y)\n",
        "    print('Rank: ',rank, ', coeff: ',coeff)\n",
        "    y_pred = predict(new_X, coeff[0], coeff[1:])\n",
        "    print('Rank: ',rank, ', y_pred: ',y_pred)\n",
        "    rms_error = rmse(y, y_pred)\n",
        "    rmse_collect.append(rms_error)\n",
        "    print('Rank: ',rank, ', rms_error: ',rms_error)\n",
        "    # worker processes\n",
        "    \n",
        "\n",
        "    recvbuf = None\n",
        "    recvbuf_rmse = None\n",
        "    if rank == 0:\n",
        "        # Creating a receiver buffer array\n",
        "        recvbuf = np.empty(coeff.shape[0] * size)  \n",
        "        #recvbuf_rmse = np.zeros((4,))\n",
        "\n",
        "    # Gathering the Information\n",
        "    comm.Gather(coeff, recvbuf, root = 0)\n",
        "    #rmse_data = comm.Gather(rmse, recvbuf_rmse, root = 0)\n",
        "    # Display the result\n",
        "    if rank == 0:\n",
        "        print('Rank: ',rank, ', recvbuf received: ',recvbuf)\n",
        "        print('rmse mean=',sum(rmse_collect)/len(rmse_collect))\n",
        "\n",
        "    \n",
        "main()\n",
        "\n"
      ],
      "metadata": {
        "id": "Vnn71e_D_8KI"
      },
      "id": "Vnn71e_D_8KI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mpirun --allow-run-as-root --oversubscribe -np 4 python mpi_reg.py"
      ],
      "metadata": {
        "id": "mmAEe182A7QI"
      },
      "id": "mmAEe182A7QI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tl_oBWtWBFP_"
      },
      "id": "Tl_oBWtWBFP_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "living-operation"
      },
      "source": [
        "#### Implement predict using OpenMP\n",
        "\n",
        "Get the predictions for test data and calculate the test error(RMSE) by implementing the OpenMP (pymp)\n",
        "\n",
        "* Using the pymp.Parallel implement the predict function (use from above)\n",
        "\n",
        "* Call the predict function by passing test data as an argument\n",
        "\n",
        "* calculate the error (RMSE) by comparing the Actual test data and predicted test data"
      ],
      "id": "living-operation"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S65Oxt1swWdh"
      },
      "source": [
        "!pip install pymp-pypi"
      ],
      "id": "S65Oxt1swWdh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "FCDeE0bIi_Oe"
      },
      "id": "FCDeE0bIi_Oe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(X_train)"
      ],
      "metadata": {
        "id": "ReKEHeXSjD91"
      },
      "id": "ReKEHeXSjD91",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "metadata": {
        "id": "2tgaSQo6mtln"
      },
      "id": "2tgaSQo6mtln",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pymp.config.nested = True\n"
      ],
      "metadata": {
        "id": "denE-6THwr7N"
      },
      "id": "denE-6THwr7N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "incremental_array = pymp.shared.array(X_test.shape)\n",
        "start = time.perf_counter()\n",
        "print('start time', start)\n",
        "\n",
        "with pymp.Parallel(4) as p:\n",
        "    # Initialize the predicted values array for each thread\n",
        "    # This will automatically be a shared variable among all threads\n",
        "    y_pred = np.zeros_like(y_test)\n",
        "\n",
        "    # Divide the test data into chunks for each thread\n",
        "    np.copyto(incremental_array, X_test)\n",
        "    y_pred = predict(incremental_array,coeff[0], coeff[1:])\n",
        "    \n",
        "finish = time.perf_counter()\n",
        "print(f'Finished in {round(finish-start, 2)} second(s)')\n",
        "# Calculate the test error (RMSE)\n",
        "error = np.sqrt(np.mean((y_test - y_pred)**2))\n",
        "print('Test RMSE: {:.4f}'.format(error))\n"
      ],
      "metadata": {
        "id": "WFWBg6vGlYiG"
      },
      "id": "WFWBg6vGlYiG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "protecting-assets"
      },
      "source": [
        "#### Use Sklearn to compare\n",
        "\n",
        "Apply the Linear regression on the given data using sklearn package and compare with the above results\n",
        "* Split the data into train and test\n",
        "* Fit the train data and predict the test data using `sklearn Linear Regression`\n",
        "* calculate loss (RMSE) on test data and predictions and compare"
      ],
      "id": "protecting-assets"
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train, X_test, y_test = train_test_split(X_copy,y_copy, test_size=0.3)"
      ],
      "metadata": {
        "id": "hchXSUJmqjwV"
      },
      "id": "hchXSUJmqjwV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, y_train.shape"
      ],
      "metadata": {
        "id": "PwpJUYfrqmgX"
      },
      "id": "PwpJUYfrqmgX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "applicable-tyler"
      },
      "source": [
        "lr = LinearRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "y_pred = lr.predict(X_test)\n",
        "y_pred"
      ],
      "id": "applicable-tyler",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rmse_score = rmse(y_test, y_pred)\n",
        "rmse_score"
      ],
      "metadata": {
        "id": "uwOX0HNfrGZt"
      },
      "id": "uwOX0HNfrGZt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V_7lGMjnrR0v"
      },
      "id": "V_7lGMjnrR0v",
      "execution_count": null,
      "outputs": []
    }
  ]
}